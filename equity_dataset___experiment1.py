# -*- coding: utf-8 -*-
"""Equity Dataset ---> Experiment1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1auLnXa7Lnc7f1wQ-ZRyKvS9sV5NFy44U
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
file_path = '/content/q1_2025_all.csv'
df = pd.read_csv(file_path)

# Inspect structure
print("Columns:", df.columns.tolist())
print(df.head())

# Parse date
df['Date'] = pd.to_datetime(df['Date'])

# Composite score based on available rank columns
rank_cols = ['McapRank', 'VolatilityRank', 'TurnRank', 'PriceRank']
df['CompositeScore'] = df[rank_cols].mean(axis=1)

# Get latest CompositeScore for each Ticker
latest_scores = df.sort_values('Date').groupby('Ticker').tail(1)
top_tickers = latest_scores.sort_values('CompositeScore').head(10)['Ticker'].tolist()

print("Top 10 Tickers by Composite Score:", top_tickers)

# Simulate fake price series for these top tickers
# (You can replace this with real price data later)
np.random.seed(42)
dates = pd.date_range(start="2025-01-01", periods=100, freq='D')
sim_prices = {
    ticker: np.cumprod(1 + np.random.normal(0.001, 0.02, size=len(dates))) * 100
    for ticker in top_tickers
}
price_df = pd.DataFrame(sim_prices, index=dates)

# Calculate returns
returns_df = price_df.pct_change().dropna()

#  Plot price trends
price_df.plot(title='Simulated Equity Prices (Top Composite Ranked)', figsize=(14, 6))
plt.grid(True)
plt.show()

# Summary
print("Tickers used:", top_tickers)
print("Returns shape:", returns_df.shape)

import pandas as pd

df = pd.read_csv('/content/q1_2025_all.csv')
print(df.columns.tolist())

"""#Run Evolutionary Algorithm (EA) on Simulated Price Data
Weâ€™ll:

Use chromosomes as asset weight vectors (real-valued)

Maximize Sharpe Ratio

Simulate crossover, mutation, and selection


"""

import random

tickers = price_df.columns.tolist()
num_assets = len(tickers)

# --- Sharpe Ratio Fitness ---
def evaluate_portfolio(weights, returns=returns_df, risk_free_rate=0.0):
    weights = np.array(weights)
    port_return = np.sum(returns.mean() * weights) * 252  # Annualized return
    port_std = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))
    sharpe = (port_return - risk_free_rate) / port_std if port_std != 0 else 0
    return sharpe, port_return, port_std

# --- Generate Initial Population ---
def generate_population(pop_size=50):
    return [np.random.dirichlet(np.ones(num_assets)) for _ in range(pop_size)]

# --- Selection ---
def tournament_selection(pop, fit, k=3):
    selected = []
    for _ in range(len(pop)):
        contenders = random.sample(list(zip(pop, fit)), k)
        selected.append(max(contenders, key=lambda x: x[1][0])[0])
    return selected

# --- Crossover + Mutation ---
def blend_crossover(p1, p2, alpha=0.5):
    c = []
    for i in range(len(p1)):
        d = abs(p1[i] - p2[i])
        low = min(p1[i], p2[i]) - alpha * d
        high = max(p1[i], p2[i]) + alpha * d
        c.append(np.clip(np.random.uniform(low, high), 0, 1))
    c = np.array(c)
    return c / c.sum()  # Normalize

def mutate(weights, rate=0.1, scale=0.05):
    if np.random.rand() < rate:
        noise = np.random.normal(0, scale, len(weights))
        weights += noise
        weights = np.clip(weights, 0, 1)
        weights /= weights.sum()
    return weights

# --- Evolution Loop ---
def evolve(pop_size=50, generations=30):
    pop = generate_population(pop_size)
    history = []

    for gen in range(generations):
        fitness = [evaluate_portfolio(w) for w in pop]
        best = max(fitness, key=lambda x: x[0])
        history.append(best)

        print(f"Gen {gen+1}: Sharpe={best[0]:.3f}, Return={best[1]:.2%}, Vol={best[2]:.2%}")
        selected = tournament_selection(pop, fitness)
        next_gen = []

        for i in range(0, pop_size, 2):
            p1, p2 = selected[i], selected[(i + 1) % pop_size]
            c1 = mutate(blend_crossover(p1, p2))
            c2 = mutate(blend_crossover(p2, p1))
            next_gen.extend([c1, c2])

        pop = next_gen[:pop_size]

    return pop, history

final_pop, fitness_hist = evolve(pop_size=50, generations=30)

"""#Plot Fitness Over Time"""

sharpes = [x[0] for x in fitness_hist]
plt.plot(sharpes)
plt.title("Best Sharpe Ratio per Generation")
plt.xlabel("Generation")
plt.ylabel("Sharpe Ratio")
plt.grid(True)
plt.show()

"""#Cluster or Visualize Composite Score vs Rank Components"""

import seaborn as sns

# Get latest rank snapshot
latest_ranks = df.sort_values('Date').groupby('Ticker').tail(1)

# ðŸ“ˆ CompositeScore vs Components
plt.figure(figsize=(12, 6))
for col in ['McapRank', 'VolatilityRank', 'TurnRank', 'PriceRank']:
    sns.regplot(data=latest_ranks, x=col, y='CompositeScore', label=col)

plt.title("Composite Score vs Rank Components")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Multi-Objective EA (Return â†‘, Volatility â†“)
We'll use a simplified NSGA-II style algorithm to evolve portfolios based on:

Objective 1: Maximize Return

Objective 2: Minimize Volatility

#Step 1: Evaluate Objectives
"""

def evaluate_objectives(weights):
    weights = np.array(weights)
    ret = np.sum(returns_df.mean() * weights) * 252
    vol = np.sqrt(np.dot(weights.T, np.dot(returns_df.cov() * 252, weights)))
    return ret, vol

"""# Step 2: Non-Dominated Sorting (Pareto Fronts)

"""

def dominates(a, b):
    return (a[0] >= b[0] and a[1] <= b[1]) and (a[0] > b[0] or a[1] < b[1])

def fast_non_dominated_sort(objectives):
    fronts = [[]]
    domination_counts = [0] * len(objectives)
    dominated_sets = [[] for _ in objectives]

    for p in range(len(objectives)):
        for q in range(len(objectives)):
            if dominates(objectives[p], objectives[q]):
                dominated_sets[p].append(q)
            elif dominates(objectives[q], objectives[p]):
                domination_counts[p] += 1
        if domination_counts[p] == 0:
            fronts[0].append(p)

    i = 0
    while fronts[i]:
        next_front = []
        for p in fronts[i]:
            for q in dominated_sets[p]:
                domination_counts[q] -= 1
                if domination_counts[q] == 0:
                    next_front.append(q)
        i += 1
        fronts.append(next_front)
    return fronts[:-1]

"""#Step 3: EA Loop (NSGA Style)"""

def evolve_nsga(pop_size=50, generations=30):
    pop = generate_population(pop_size)
    all_objs = []
    pareto_fronts = []

    for gen in range(generations):
        objs = [evaluate_objectives(chrom) for chrom in pop]
        fronts = fast_non_dominated_sort(objs)
        best_front = fronts[0]
        pareto_fronts.append(best_front)
        all_objs.append(objs)

        print(f"Gen {gen+1}: Pareto front size = {len(best_front)}")

        # Selection: keep best front and sample rest via tournament
        survivors = [pop[i] for i in best_front]
        selected = tournament_selection(pop, [(r, -v, 0) for r, v in objs])

        next_gen = []
        for i in range(0, pop_size, 2):
            p1, p2 = selected[i], selected[(i + 1) % pop_size]
            c1 = mutate(blend_crossover(p1, p2))
            c2 = mutate(blend_crossover(p2, p1))
            next_gen.extend([c1, c2])

        pop = next_gen[:pop_size]

    return pop, all_objs, pareto_fronts

pop_nsga, all_objs, pareto_fronts = evolve_nsga(pop_size=50, generations=30)

"""#Plot Final Pareto Front"""

# Extract final generation's Pareto front
final_objs = all_objs[-1]
pf_idx = pareto_fronts[-1]
pf_returns = [final_objs[i][0] for i in pf_idx]
pf_vols = [final_objs[i][1] for i in pf_idx]

plt.figure(figsize=(10, 6))
plt.scatter(*zip(*final_objs), color='lightgray', label='All Portfolios')
plt.scatter(pf_vols, pf_returns, color='red', s=70, edgecolor='black', label='Pareto Front')
plt.xlabel("Volatility")
plt.ylabel("Return")
plt.title("NSGA-II: Pareto Front in Risk-Return Space")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Part 2: KMeans Clustering on Final EA Portfolios

#Cluster Pareto Portfolios
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Extract weights of Pareto portfolios
pareto_weights = [pop_nsga[i] for i in pf_idx]
X_scaled = StandardScaler().fit_transform(pareto_weights)

# KMeans (try 3 clusters)
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X_scaled)

# Append to dataframe
import pandas as pd
pareto_df = pd.DataFrame(pareto_weights, columns=price_df.columns)
pareto_df['Return'] = pf_returns
pareto_df['Volatility'] = pf_vols
pareto_df['Cluster'] = labels

"""#Visualize Clusters in Risk-Return Space"""

plt.figure(figsize=(10, 6))
for label in sorted(pareto_df['Cluster'].unique()):
    cluster_data = pareto_df[pareto_df['Cluster'] == label]
    plt.scatter(cluster_data['Volatility'], cluster_data['Return'], label=f"Cluster {label}")

plt.xlabel("Volatility")
plt.ylabel("Return")
plt.title("KMeans Clustering of Pareto Portfolios")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Part 1: Analyze Average Weights Per Cluster
This helps you understand:

Which assets dominate in each strategy cluster

Typical allocation behavior in each group

Code to Analyze Cluster Profiles
"""

# Average weights per cluster (drop return/vol/cluster for weight analysis)
weight_columns = price_df.columns.tolist()
cluster_profiles = pareto_df.groupby('Cluster')[weight_columns].mean()

# Round for readability
print("Cluster-wise Average Portfolio Weights:")
display(cluster_profiles.round(3))

"""#Visualize Each Clusterâ€™s Portfolio Style"""

import seaborn as sns

for cluster_id in cluster_profiles.index:
    plt.figure(figsize=(8, 4))
    sns.barplot(x=cluster_profiles.columns, y=cluster_profiles.loc[cluster_id].values)
    plt.title(f"Cluster {cluster_id} - Avg Portfolio Weights")
    plt.xticks(rotation=45)
    plt.ylabel("Average Weight")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""#Use Clusters to Guide Final Portfolio Selection

Strategy A: Pick Cluster with Highest Average Return
"""

cluster_returns = pareto_df.groupby('Cluster')['Return'].mean()
best_cluster_id = cluster_returns.idxmax()

print(f"ðŸŽ¯ Best-performing cluster: {best_cluster_id} (Avg Return = {cluster_returns[best_cluster_id]:.2%})")

# Pick portfolio with best Sharpe in that cluster
best_portfolio = pareto_df[pareto_df['Cluster'] == best_cluster_id].sort_values(
    by='Return', ascending=False).iloc[0]

# Show weights
print("Optimal Portfolio Weights from Best Cluster:")
print(best_portfolio[weight_columns].round(3))

"""#Time-Varying Portfolio Optimization using EA

Objective:
Apply EA on rolling windows (e.g., 60 days)

Generate optimal portfolio per window

Track evolution of:

Optimal weights

Sharpe ratio

Return/volatility drift

Step 1: Define Rolling EA Function
"""

def rolling_ea_optimization(returns_df, window_size=60, step_size=10, generations=30, pop_size=30):
    dates = returns_df.index
    result_weights = []
    result_sharpes = []
    result_dates = []

    for start in range(0, len(dates) - window_size, step_size):
        end = start + window_size
        window_returns = returns_df.iloc[start:end]

        print(f"ðŸŒ€ Rolling Window: {dates[start].date()} â†’ {dates[end-1].date()}")

        # EA for this window
        def fitness(w):
            return evaluate_portfolio(w, window_returns)[0]

        pop = generate_population(pop_size)
        for _ in range(generations):
            fitness_vals = [fitness(w) for w in pop]
            selected = tournament_selection(pop, [(f, 0, 0) for f in fitness_vals])
            next_gen = []
            for i in range(0, pop_size, 2):
                p1, p2 = selected[i], selected[(i + 1) % pop_size]
                c1 = mutate(blend_crossover(p1, p2))
                c2 = mutate(blend_crossover(p2, p1))
                next_gen.extend([c1, c2])
            pop = next_gen[:pop_size]

        # Evaluate final best
        final_fitness = [evaluate_portfolio(w, window_returns) for w in pop]
        best_idx = np.argmax([f[0] for f in final_fitness])
        best_weights = pop[best_idx]
        best_sharpe = final_fitness[best_idx][0]

        result_weights.append(best_weights)
        result_sharpes.append(best_sharpe)
        result_dates.append(dates[end])

    return np.array(result_weights), np.array(result_sharpes), result_dates

"""Step 2: Run the Rolling EA"""

rolling_weights, rolling_sharpes, rolling_dates = rolling_ea_optimization(
    returns_df=returns_df,
    window_size=60,
    step_size=10,
    generations=20,
    pop_size=30
)

"""#Step 3: Plot Evolution of Sharpe Ratios

"""

plt.figure(figsize=(10, 5))
plt.plot(rolling_dates, rolling_sharpes, marker='o')
plt.title("Sharpe Ratio Over Rolling EA Windows")
plt.xlabel("Date")
plt.ylabel("Best Sharpe Ratio")
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Step 4: Plot Weight Drift Over Time (Heatmap)


"""

import seaborn as sns

rolling_df = pd.DataFrame(rolling_weights, columns=returns_df.columns, index=rolling_dates)

plt.figure(figsize=(12, 6))
sns.heatmap(rolling_df.T, cmap='YlGnBu', cbar=True)
plt.title("Portfolio Weight Drift Over Time")
plt.xlabel("Date")
plt.ylabel("Ticker")
plt.tight_layout()
plt.show()

"""#Overlay Market Regimes (Bull/Bear) on Sharpe Ratio Chart
Weâ€™ll:

Detect regimes using rolling returns or volatility

Label as:

ðŸŸ¢ Bull: Return > 0

ðŸ”´ Bear: Return < 0

Overlay colored zones on the Sharpe ratio chart


"""

# Compute rolling market return (proxy regime indicator)
market_return = returns_df.mean(axis=1).rolling(window=60).mean()
regime_dates = rolling_dates
regime_values = [market_return.loc[d] if d in market_return.index else 0 for d in rolling_dates]
regimes = ['Bull' if r > 0 else 'Bear' for r in regime_values]
colors = ['green' if r == 'Bull' else 'red' for r in regimes]

# Overlay plot
plt.figure(figsize=(12, 5))
for i in range(len(rolling_dates) - 1):
    plt.axvspan(rolling_dates[i], rolling_dates[i+1], color=colors[i], alpha=0.2)

plt.plot(rolling_dates, rolling_sharpes, color='black', marker='o', label='Sharpe Ratio')
plt.title("Sharpe Ratio Over Time with Market Regimes")
plt.xlabel("Date")
plt.ylabel("Sharpe Ratio")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#Compare Rolling EA vs Static EA Portfolio
Weâ€™ll:

Run one static EA on entire returns_df

Calculate cumulative return for both portfolios:

Static EA portfolio

Rolling EA portfolios (rebalanced at each step)


"""

# Run EA on full data
full_returns = returns_df.copy()
pop = generate_population(30)
for _ in range(20):
    fit = [evaluate_portfolio(w, full_returns)[0] for w in pop]
    selected = tournament_selection(pop, [(f, 0, 0) for f in fit])
    next_gen = [mutate(blend_crossover(selected[i], selected[(i+1)%30])) for i in range(30)]
    pop = next_gen

best_static = pop[np.argmax([evaluate_portfolio(w, full_returns)[0] for w in pop])]

"""#Compare Cumulative Performance

\

"""

# Rolling portfolio return (step-based rebalancing)
rolling_cum = []
static_cum = []
weights_static = best_static
cum_return_rolling = 1
cum_return_static = 1

for i, d in enumerate(rolling_dates[:-1]):
    next_d = rolling_dates[i+1]
    window = returns_df.loc[d:next_d]

    # Rolling EA portfolio for this window
    w_roll = rolling_weights[i]
    ret_roll = (window @ w_roll).add(1).prod()

    # Static portfolio
    ret_static = (window @ weights_static).add(1).prod()

    cum_return_rolling *= ret_roll
    cum_return_static *= ret_static

    rolling_cum.append(cum_return_rolling)
    static_cum.append(cum_return_static)

plt.figure(figsize=(10, 5))
plt.plot(rolling_dates[1:], rolling_cum, label='Rolling EA Portfolio', marker='o')
plt.plot(rolling_dates[1:], static_cum, label='Static EA Portfolio', marker='x')
plt.title("Cumulative Return: Rolling vs Static EA")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#Drift-Aware Evolutionary Optimization (Rolling EA + Drift Penalty)
 Goal:
Modify the fitness function in your rolling EA loop to penalize changes in portfolio weights vs previous window.

#Step 1: Drift Penalty Function
"""

def compute_drift_penalty(current_weights, previous_weights, drift_strength=0.5):
    # L1 norm difference
    drift = np.sum(np.abs(current_weights - previous_weights))
    return -drift_strength * drift

"""#Step 2: Modify Rolling EA Function to Include Drift
Update your rolling_ea_optimization() by tracking previous_weights:



"""

def rolling_ea_with_drift(returns_df, window_size=60, step_size=10, generations=30, pop_size=30, drift_strength=0.5):
    dates = returns_df.index
    result_weights = []
    result_sharpes = []
    result_dates = []
    previous_weights = np.ones(returns_df.shape[1]) / returns_df.shape[1]  # Equal-weight start

    for start in range(0, len(dates) - window_size, step_size):
        end = start + window_size
        window_returns = returns_df.iloc[start:end]

        print(f"ðŸŒ€ Rolling Window: {dates[start].date()} â†’ {dates[end-1].date()}")

        def fitness(w):
            sharpe = evaluate_portfolio(w, window_returns)[0]
            drift_penalty = compute_drift_penalty(w, previous_weights, drift_strength)
            return sharpe + drift_penalty  # penalize change

        pop = generate_population(pop_size)
        for _ in range(generations):
            fitness_vals = [fitness(w) for w in pop]
            selected = tournament_selection(pop, [(f, 0, 0) for f in fitness_vals])
            next_gen = []
            for i in range(0, pop_size, 2):
                p1, p2 = selected[i], selected[(i + 1) % pop_size]
                c1 = mutate(blend_crossover(p1, p2))
                c2 = mutate(blend_crossover(p2, p1))
                next_gen.extend([c1, c2])
            pop = next_gen[:pop_size]

        final_fitness = [evaluate_portfolio(w, window_returns) for w in pop]
        best_idx = np.argmax([f[0] for f in final_fitness])
        best_weights = pop[best_idx]
        best_sharpe = final_fitness[best_idx][0]

        result_weights.append(best_weights)
        result_sharpes.append(best_sharpe)
        result_dates.append(dates[end])

        previous_weights = best_weights  # update drift reference

    return np.array(result_weights), np.array(result_sharpes), result_dates

"""#Run Drift-Aware EA"""

drift_weights, drift_sharpes, drift_dates = rolling_ea_with_drift(
    returns_df=returns_df,
    window_size=60,
    step_size=10,
    generations=20,
    pop_size=30,
    drift_strength=0.3  # adjust to control penalty severity
)

plt.figure(figsize=(12, 5))
plt.plot(rolling_dates, rolling_sharpes, label='Without Drift Penalty', marker='o')
plt.plot(drift_dates, drift_sharpes, label='With Drift Penalty', marker='x')
plt.title("Sharpe Ratio Over Time: Drift vs No Drift")
plt.xlabel("Date")
plt.ylabel("Sharpe Ratio")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#Visualize Drift Magnitude Over Time"""

drifts = [np.sum(np.abs(drift_weights[i] - drift_weights[i-1]))
          for i in range(1, len(drift_weights))]

plt.figure(figsize=(10, 4))
plt.plot(drift_dates[1:], drifts, marker='s', color='orange')
plt.title("Portfolio Drift Magnitude (L1 Distance) Between Periods")
plt.xlabel("Date")
plt.ylabel("Weight Change (Drift)")
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#Explainable AI (XAI) for EA Portfolios with SHAP
 Goal:
Use SHAP values to explain:

Why certain assets are consistently assigned high weights

How features (returns, vol, correlations) influence portfolio allocation

#Step 1: Prepare Feature Matrix for SHAP
Letâ€™s assume we use basic asset-level stats (per rolling window) as features:
"""

def get_features_for_window(window_returns):
    features = pd.DataFrame({
        'MeanReturn': window_returns.mean() * 252,
        'Volatility': window_returns.std() * np.sqrt(252),
        'Skewness': window_returns.skew(),
        'Kurtosis': window_returns.kurt(),
        'MaxDrawdown': window_returns.apply(lambda r: (r.cumsum().cummax() - r.cumsum()).max())
    })
    return features

"""#Step 2: Train a Model to Predict EA-Assigned Weights"""

from sklearn.ensemble import RandomForestRegressor
import shap

# Example window
window = returns_df.iloc[-60:]
weights_ea = best_portfolio[price_df.columns]  # from EA
features = get_features_for_window(window)

# Train model
model = RandomForestRegressor()
model.fit(features, weights_ea)

# SHAP explanation
explainer = shap.Explainer(model, features)
shap_values = explainer(features)

# Plot
shap.plots.bar(shap_values, max_display=10)

"""#Part 2: CVaR and Drawdown-Based EA Fitness

#Step 1: Define CVaR Loss
"""

def cvar_fitness(weights, returns, alpha=0.05):
    port_ret = returns @ weights
    port_ret_sorted = np.sort(port_ret)
    var = np.percentile(port_ret_sorted, 100 * alpha)
    cvar = port_ret_sorted[port_ret_sorted <= var].mean()
    return -cvar  # We minimize CVaR â†’ negative fitness

"""#Step 2: Define Max Drawdown Penalty

"""

def max_drawdown(portfolio_returns):
    cum_returns = (1 + portfolio_returns).cumprod()
    peak = cum_returns.cummax()
    drawdown = (cum_returns - peak) / peak
    return drawdown.min()

"""#Step 3: Combined Risk-Averse Fitness Function

"""

def risk_aware_fitness(weights, returns, lam_cvar=1.0, lam_mdd=0.5):
    w = np.array(weights)
    ret = np.sum(returns.mean() * w) * 252
    cvar = -cvar_fitness(w, returns)
    mdd = max_drawdown(returns @ w)
    score = ret - lam_cvar * cvar - lam_mdd * abs(mdd)
    return score

"""#Apply This Fitness in EA
Inside your EA loop for a window:
"""

def fitness(w):
    return risk_aware_fitness(w, window_returns)

# Assume these are your final optimized weights (replace with real outputs)
best_portfolio_sharpe = np.random.dirichlet(np.ones(len(price_df.columns)))
best_portfolio_risk = np.random.dirichlet(np.ones(len(price_df.columns)))

tickers = price_df.columns.tolist()

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(len(tickers))
width = 0.35

plt.figure(figsize=(12, 5))
plt.bar(x - width/2, best_portfolio_sharpe, width, label='Sharpe EA')
plt.bar(x + width/2, best_portfolio_risk, width, label='CVaR+Drawdown EA')
plt.xticks(x, tickers, rotation=45)
plt.ylabel("Weight")
plt.title("Final Portfolio Weights")
plt.legend()
plt.tight_layout()
plt.grid(True)
plt.show()

"""#Step 2: Plot Cumulative Returns"""

returns_sharpe = returns_df @ best_portfolio_sharpe
returns_risk = returns_df @ best_portfolio_risk

cum_sharpe = (1 + returns_sharpe).cumprod()
cum_risk = (1 + returns_risk).cumprod()

plt.figure(figsize=(12, 5))
plt.plot(cum_sharpe, label='Sharpe EA')
plt.plot(cum_risk, label='CVaR+Drawdown EA')
plt.title("Cumulative Returns")
plt.xlabel("Date")
plt.ylabel("Portfolio Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Step 3: Worst 5-Day Drop (Rolling Drawdown)

"""

roll_window = 5
rolling_dd_sharpe = returns_sharpe.rolling(roll_window).sum().min()
rolling_dd_risk = returns_risk.rolling(roll_window).sum().min()

print(f"ðŸ“‰ Worst 5-Day Drop - Sharpe EA: {rolling_dd_sharpe:.2%}")
print(f"ðŸ“‰ Worst 5-Day Drop - CVaR+Drawdown EA: {rolling_dd_risk:.2%}")

