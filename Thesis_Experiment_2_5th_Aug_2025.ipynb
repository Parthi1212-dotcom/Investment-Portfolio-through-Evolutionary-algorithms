{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaX516gvgTv7X7veZ4jP6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parthi1212-dotcom/Investment-Portfolio-through-Evolutionary-algorithms/blob/main/Thesis_Experiment_2_5th_Aug_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF45dKZ-QoN4",
        "outputId": "63e56551-d689-4261-846a-a66622add0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and processing data for: AAPL, MSFT, TSLA...\n",
            "Processing AAPL...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4276153667.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  stock_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing MSFT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4276153667.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  stock_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TSLA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4276153667.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  stock_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset successfully created and saved as 'new_portfolio_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_financial_dataset(tickers, start_date, end_date, output_filename=\"new_portfolio_dataset.csv\"):\n",
        "    \"\"\"\n",
        "    Downloads historical stock data, calculates financial metrics and trading signals,\n",
        "    and saves the combined data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        tickers (list): A list of stock tickers (e.g., ['AAPL', 'MSFT']).\n",
        "        start_date (str): The start date for the data in 'YYYY-MM-DD' format.\n",
        "        end_date (str): The end date for the data in 'YYYY-MM-DD' format.\n",
        "        output_filename (str): The name of the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    all_tickers_df = pd.DataFrame()\n",
        "\n",
        "    print(f\"Downloading and processing data for: {', '.join(tickers)}...\")\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Processing {ticker}...\")\n",
        "\n",
        "        # --- 1. Download Historical Data ---\n",
        "        # Using yfinance to download daily stock data.\n",
        "        stock_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "        if stock_df.empty:\n",
        "            print(f\"Could not download data for {ticker}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- 2. Calculate Basic Financial Metrics ---\n",
        "        # Using 'Adj Close' as it accounts for dividends and stock splits.\n",
        "        stock_df['Price'] = stock_df['Close']\n",
        "\n",
        "        # Calculate daily return as a percentage.\n",
        "        stock_df['Return'] = stock_df['Price'].pct_change()\n",
        "\n",
        "        # Calculate the absolute price change from the previous day.\n",
        "        stock_df['Delta_Price'] = stock_df['Price'].diff()\n",
        "\n",
        "        # Calculate volatility as the rolling standard deviation of returns (e.g., 30-day window).\n",
        "        stock_df['Volatility'] = stock_df['Return'].rolling(window=30).std()\n",
        "\n",
        "        # --- 3. Generate a Sample Trading Signal ---\n",
        "        # This is a demonstration strategy. The original file's signals cannot be replicated.\n",
        "        # Strategy: Buy when the 20-day moving average crosses above the 50-day moving average.\n",
        "        ma_short = stock_df['Price'].rolling(window=20).mean()\n",
        "        ma_long = stock_df['Price'].rolling(window=50).mean()\n",
        "\n",
        "        # Generate signal: +1 for Buy, -1 for Sell.\n",
        "        stock_df['Signal'] = 0\n",
        "        stock_df.loc[ma_short > ma_long, 'Signal'] = 1\n",
        "        stock_df.loc[ma_short < ma_long, 'Signal'] = -1\n",
        "\n",
        "        # Create a descriptive label from the signal.\n",
        "        stock_df['Label'] = stock_df['Signal'].map({1: 'Buy', -1: 'Sell', 0: 'Hold'})\n",
        "\n",
        "        # --- 4. Add Fundamental Data (P/E Ratio) ---\n",
        "        # Fetching current P/E ratio. Note: Historical daily P/E is not easily available.\n",
        "        stock_info = yf.Ticker(ticker).info\n",
        "        pe_ratio = stock_info.get('trailingPE', np.nan) # Use 'trailingPE' or 'forwardPE'\n",
        "        stock_df['P/E Ratio'] = pe_ratio\n",
        "\n",
        "        # --- 5. Finalize DataFrame ---\n",
        "        # Add the ticker symbol to the dataframe.\n",
        "        stock_df['Ticker'] = ticker\n",
        "\n",
        "        # Append this ticker's data to the main dataframe.\n",
        "        all_tickers_df = pd.concat([all_tickers_df, stock_df])\n",
        "\n",
        "    # --- 6. Clean and Save Data ---\n",
        "    # Select and reorder columns to be clear and organized.\n",
        "    final_columns = [\n",
        "        'Date', 'Price', 'Ticker', 'Return', 'Delta_Price',\n",
        "        'Volatility', 'Signal', 'Label', 'P/E Ratio',\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume'\n",
        "    ]\n",
        "\n",
        "    # Reset index to make 'Date' a column.\n",
        "    all_tickers_df.reset_index(inplace=True)\n",
        "\n",
        "    # Filter for the desired columns and drop rows with missing values from rolling calculations.\n",
        "    final_df = all_tickers_df[final_columns].dropna()\n",
        "\n",
        "    # Save the final dataframe to a CSV file.\n",
        "    final_df.to_csv(output_filename, index=False)\n",
        "    print(f\"\\nDataset successfully created and saved as '{output_filename}'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    # Define the stocks you want to analyze.\n",
        "    TICKERS_TO_PROCESS = ['AAPL', 'MSFT', 'TSLA']\n",
        "\n",
        "    # Define the date range for the historical data.\n",
        "    START_DATE = '2016-01-01'\n",
        "    END_DATE = '2020-07-22' # Use a recent date for the end.\n",
        "\n",
        "    # --- Run the function ---\n",
        "    create_financial_dataset(TICKERS_TO_PROCESS, START_DATE, END_DATE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Calculates and adds technical indicators (RSI, MACD, Bollinger Bands)\n",
        "    to the input dataframe.\n",
        "    \"\"\"\n",
        "    # RSI (Relative Strength Index)\n",
        "    delta = df['Price'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD (Moving Average Convergence Divergence)\n",
        "    exp1 = df['Price'].ewm(span=12, adjust=False).mean()\n",
        "    exp2 = df['Price'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = exp1 - exp2\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    ma20 = df['Price'].rolling(window=20).mean()\n",
        "    std20 = df['Price'].rolling(window=20).std()\n",
        "    df['Bollinger_Upper'] = ma20 + (std20 * 2)\n",
        "    df['Bollinger_Lower'] = ma20 - (std20 * 2)\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# Configuration\n",
        "tickers = ['AAPL', 'TSLA', 'MSFT']\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2025-08-01'\n",
        "\n",
        "# 1. Download price data (MultiIndex)\n",
        "print(\"Downloading historical data...\")\n",
        "data = yf.download(tickers, start=start_date, end=end_date)\n",
        "adj_close = data['Close']\n",
        "print(\"Data download complete.\")\n",
        "\n",
        "# 2. Get P/E ratios\n",
        "print(\"Fetching P/E ratios...\")\n",
        "pe_ratios = {}\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "        pe = info.get('trailingPE', None)\n",
        "        pe_ratios[ticker] = pe\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch PE for {ticker}: {e}\")\n",
        "        pe_ratios[ticker] = None\n",
        "print(\"P/E fetch complete.\")\n",
        "\n",
        "# 3. Create and enrich dataset\n",
        "print(\"Creating and enriching dataset for each ticker...\")\n",
        "df_list = []\n",
        "for ticker in adj_close.columns:\n",
        "    # Create initial dataframe with your metrics\n",
        "    df = adj_close[[ticker]].copy()\n",
        "    df.columns = ['Price']\n",
        "    df['Ticker'] = ticker\n",
        "    df['Return'] = df['Price'].pct_change()\n",
        "    df['Delta_Price'] = df['Price'].diff()\n",
        "    df['Volatility'] = df['Return'].rolling(window=20).std() * np.sqrt(252)\n",
        "    df['Signal'] = np.where(df['Return'] > 0, 1, -1)\n",
        "    df['Label'] = np.where(df['Return'] > 0.01, 'Buy', 'Sell')\n",
        "    df['P/E Ratio'] = pe_ratios.get(ticker, None)\n",
        "\n",
        "    # *** ADD THE NEW TECHNICAL INDICATORS ***\n",
        "    df = add_technical_indicators(df)\n",
        "\n",
        "    df.reset_index(inplace=True)\n",
        "    df_list.append(df)\n",
        "print(\"Enrichment complete.\")\n",
        "\n",
        "# 4. Combine and save\n",
        "print(\"Combining data and saving to CSV...\")\n",
        "final_df = pd.concat(df_list)\n",
        "final_df.dropna(inplace=True)\n",
        "final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the final, enriched dataset\n",
        "output_filename = \"portfolio_dataset_enriched.csv\"\n",
        "final_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nSuccess! Enriched dataset saved as '{output_filename}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7dltdolQyPl",
        "outputId": "12e233ae-b162-4b2d-85d4-834331fae0f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading historical data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3734662601.py:40: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(tickers, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  3 of 3 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data download complete.\n",
            "Fetching P/E ratios...\n",
            "P/E fetch complete.\n",
            "Creating and enriching dataset for each ticker...\n",
            "Enrichment complete.\n",
            "Combining data and saving to CSV...\n",
            "\n",
            "Success! Enriched dataset saved as 'portfolio_dataset_enriched.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the enriched dataset\n",
        "try:\n",
        "    df = pd.read_csv('/content/portfolio_dataset_enriched.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Enriched dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'portfolio_dataset_enriched.csv' not found.\")\n",
        "    print(\"Please ensure you have run the previous script to generate this file.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Advanced Visualization ---\n",
        "print(\"\\nGenerating advanced visualizations...\")\n",
        "\n",
        "# a) RSI Distribution by Signal\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(data=df, x='Ticker', y='RSI', hue='Label')\n",
        "plt.title('RSI Distribution for Buy/Sell Signals')\n",
        "plt.grid(True, axis='y')\n",
        "plt.savefig('rsi_distribution_by_signal.png')\n",
        "plt.close()\n",
        "print(\"Saved 'rsi_distribution_by_signal.png'\")\n",
        "\n",
        "# b) MACD Crossover Visualization (for a sample)\n",
        "def plot_macd_sample(df_ticker, ticker_name):\n",
        "    # Take a sample of the data to make the plot readable\n",
        "    sample_df = df_ticker.iloc[100:250]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(sample_df['Date'], sample_df['MACD'], label='MACD', color='blue')\n",
        "    plt.plot(sample_df['Date'], sample_df['MACD_Signal'], label='Signal Line', color='orange')\n",
        "\n",
        "    buy_signals = sample_df[sample_df['Signal'] == 1]\n",
        "    sell_signals = sample_df[sample_df['Signal'] == -1]\n",
        "\n",
        "    plt.scatter(buy_signals['Date'], buy_signals['MACD'], label='Buy Signal', marker='^', color='green', s=150, zorder=5)\n",
        "    plt.scatter(sell_signals['Date'], sell_signals['MACD'], label='Sell Signal', marker='v', color='red', s=150, zorder=5)\n",
        "\n",
        "    plt.title(f'MACD Crossover with Signals for {ticker_name} (Sample)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'macd_crossover_{ticker_name}.png')\n",
        "    plt.close()\n",
        "    print(f\"Saved 'macd_crossover_{ticker_name}.png'\")\n",
        "\n",
        "# Generate MACD plot for each ticker\n",
        "for ticker in df['Ticker'].unique():\n",
        "    plot_macd_sample(df[df['Ticker'] == ticker], ticker)\n",
        "\n",
        "# --- 2. Machine Learning: Signal Prediction ---\n",
        "print(\"\\nStarting Machine Learning analysis...\")\n",
        "\n",
        "all_feature_importances = pd.DataFrame()\n",
        "model_accuracies = {}\n",
        "\n",
        "for ticker in df['Ticker'].unique():\n",
        "    print(f\"\\n--- Processing Ticker: {ticker} ---\")\n",
        "\n",
        "    ticker_df = df[df['Ticker'] == ticker].copy()\n",
        "\n",
        "    # a) Feature Engineering & Selection\n",
        "    features = [\n",
        "        'RSI', 'MACD', 'MACD_Signal', 'Bollinger_Upper',\n",
        "        'Bollinger_Lower', 'Volatility'\n",
        "    ]\n",
        "\n",
        "    # The target is the signal of the *next* day\n",
        "    ticker_df['Target'] = ticker_df['Signal'].shift(-1)\n",
        "\n",
        "    # Drop rows with NaN values (from indicator calculations and target shift)\n",
        "    ticker_df.dropna(inplace=True)\n",
        "\n",
        "    X = ticker_df[features]\n",
        "    y = ticker_df['Target']\n",
        "\n",
        "    # b) Train-Test Split (Time-series aware)\n",
        "    # We use the first 80% of the data for training and the last 20% for testing.\n",
        "    split_index = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:split_index], X[split_index:]\n",
        "    y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "    print(f\"Training data size: {len(X_train)}, Testing data size: {len(X_test)}\")\n",
        "\n",
        "    # c) Model Training\n",
        "    # RandomForest is a great choice as it's robust and handles complex interactions.\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # d) Prediction & Evaluation\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    model_accuracies[ticker] = accuracy\n",
        "\n",
        "    print(f\"Test Set Accuracy for {ticker}: {accuracy:.4f}\")\n",
        "    # print(\"\\nClassification Report:\")\n",
        "    # print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # e) Feature Importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': features,\n",
        "        'Importance': model.feature_importances_,\n",
        "        'Ticker': ticker\n",
        "    })\n",
        "    all_feature_importances = pd.concat([all_feature_importances, importance_df])\n",
        "\n",
        "# --- 3. Plot Overall Feature Importance ---\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=all_feature_importances, x='Importance', y='Feature', hue='Ticker')\n",
        "plt.title('Feature Importance by Ticker for Predicting Next Day Signal')\n",
        "plt.grid(True, axis='x')\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "print(\"\\nSaved 'feature_importance.png'\")\n",
        "\n",
        "print(\"\\n--- Summary of Model Accuracies ---\")\n",
        "for ticker, acc in model_accuracies.items():\n",
        "    print(f\"{ticker}: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFUB3Fl5RS7J",
        "outputId": "beb663ac-36a9-4423-c517-da15f921f0bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enriched dataset loaded successfully.\n",
            "\n",
            "Generating advanced visualizations...\n",
            "Saved 'rsi_distribution_by_signal.png'\n",
            "Saved 'macd_crossover_AAPL.png'\n",
            "Saved 'macd_crossover_MSFT.png'\n",
            "Saved 'macd_crossover_TSLA.png'\n",
            "\n",
            "Starting Machine Learning analysis...\n",
            "\n",
            "--- Processing Ticker: AAPL ---\n",
            "Training data size: 1104, Testing data size: 277\n",
            "Test Set Accuracy for AAPL: 0.4693\n",
            "\n",
            "--- Processing Ticker: MSFT ---\n",
            "Training data size: 1104, Testing data size: 277\n",
            "Test Set Accuracy for MSFT: 0.4838\n",
            "\n",
            "--- Processing Ticker: TSLA ---\n",
            "Training data size: 1104, Testing data size: 277\n",
            "Test Set Accuracy for TSLA: 0.5523\n",
            "\n",
            "Saved 'feature_importance.png'\n",
            "\n",
            "--- Summary of Model Accuracies ---\n",
            "AAPL: 46.93%\n",
            "MSFT: 48.38%\n",
            "TSLA: 55.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset for large vs small cap company"
      ],
      "metadata": {
        "id": "5ua5MZ8MlDEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Calculates and adds technical indicators (RSI, MACD, Bollinger Bands)\n",
        "    to the input dataframe.\n",
        "    \"\"\"\n",
        "    # RSI (Relative Strength Index)\n",
        "    delta = df['Price'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD (Moving Average Convergence Divergence)\n",
        "    exp1 = df['Price'].ewm(span=12, adjust=False).mean()\n",
        "    exp2 = df['Price'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = exp1 - exp2\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    ma20 = df['Price'].rolling(window=20).mean()\n",
        "    std20 = df['Price'].rolling(window=20).std()\n",
        "    df['Bollinger_Upper'] = ma20 + (std20 * 2)\n",
        "    df['Bollinger_Lower'] = ma20 - (std20 * 2)\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# Configuration\n",
        "tickers = [\n",
        "    # Large-Cap\n",
        "    'AAPL', 'MSFT', 'TSLA',\n",
        "    # Small/Mid-Cap\n",
        "    'PLTR', 'APPS', 'ETSY', 'PLUG', 'CRSP'\n",
        "]\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2025-08-01'\n",
        "output_filename = \"large_vs_small_cap_dataset.csv\"\n",
        "\n",
        "\n",
        "# 1. Download price data (MultiIndex)\n",
        "print(f\"Downloading historical data for {len(tickers)} tickers...\")\n",
        "data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "adj_close = data['Close']\n",
        "print(\"Data download complete.\")\n",
        "\n",
        "# 2. Get P/E ratios\n",
        "print(\"Fetching P/E ratios...\")\n",
        "pe_ratios = {}\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "        pe = info.get('trailingPE', None)\n",
        "        pe_ratios[ticker] = pe\n",
        "    except Exception as e:\n",
        "        print(f\"--> Failed to fetch PE for {ticker}: {e}\")\n",
        "        pe_ratios[ticker] = None\n",
        "print(\"P/E fetch complete.\")\n",
        "\n",
        "# 3. Create and enrich dataset\n",
        "print(\"Creating and enriching dataset for each ticker...\")\n",
        "df_list = []\n",
        "for ticker in adj_close.columns:\n",
        "    # Create initial dataframe with your metrics\n",
        "    df = adj_close[[ticker]].copy()\n",
        "    df.columns = ['Price']\n",
        "    df['Ticker'] = ticker\n",
        "    df['Return'] = df['Price'].pct_change()\n",
        "    df['Delta_Price'] = df['Price'].diff()\n",
        "    df['Volatility'] = df['Return'].rolling(window=20).std() * np.sqrt(252)\n",
        "    df['Signal'] = np.where(df['Return'] > 0, 1, -1)\n",
        "    df['Label'] = np.where(df['Return'] > 0.01, 'Buy', 'Sell')\n",
        "    df['P/E Ratio'] = pe_ratios.get(ticker, None)\n",
        "\n",
        "    # Add the technical indicators\n",
        "    df = add_technical_indicators(df)\n",
        "\n",
        "    df.reset_index(inplace=True)\n",
        "    df_list.append(df)\n",
        "print(\"Enrichment complete.\")\n",
        "\n",
        "# 4. Combine and save\n",
        "print(\"Combining data and saving to CSV...\")\n",
        "final_df = pd.concat(df_list)\n",
        "final_df.dropna(inplace=True)\n",
        "final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the final, enriched dataset\n",
        "final_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nSuccess! Enriched dataset with large and small-cap stocks saved as '{output_filename}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liS4qoMZSXSu",
        "outputId": "9cc7823c-69f9-4f75-8b9b-ec6023abd22c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading historical data for 8 tickers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-300789265.py:47: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data download complete.\n",
            "Fetching P/E ratios...\n",
            "P/E fetch complete.\n",
            "Creating and enriching dataset for each ticker...\n",
            "Enrichment complete.\n",
            "Combining data and saving to CSV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-300789265.py:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  final_df = pd.concat(df_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! Enriched dataset with large and small-cap stocks saved as 'large_vs_small_cap_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load Data and Define Groups ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    print(\"Please ensure the dataset file is available.\")\n",
        "    exit()\n",
        "\n",
        "# Define the groups for comparison\n",
        "large_cap_tickers = ['AAPL', 'MSFT', 'TSLA']\n",
        "small_mid_cap_tickers = ['PLTR', 'APPS', 'ETSY', 'PLUG', 'CRSP']\n",
        "\n",
        "# Add a 'Cap_Type' column for easy grouping\n",
        "df['Cap_Type'] = df['Ticker'].apply(\n",
        "    lambda x: 'Large-Cap' if x in large_cap_tickers else 'Small/Mid-Cap'\n",
        ")\n",
        "\n",
        "# --- 2. Create Comparative Visualizations ---\n",
        "print(\"Generating comparative visualizations...\")\n",
        "\n",
        "# Set up the figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "fig.suptitle('Comparative Analysis: Large-Cap vs. Small/Mid-Cap Stocks', fontsize=16)\n",
        "\n",
        "\n",
        "# a) Volatility Comparison (Boxplot)\n",
        "sns.boxplot(ax=axes[0], data=df, x='Cap_Type', y='Volatility',\n",
        "            palette={'Large-Cap': 'skyblue', 'Small/Mid-Cap': 'lightgreen'})\n",
        "axes[0].set_title('Volatility Comparison')\n",
        "axes[0].set_xlabel('Stock Group')\n",
        "axes[0].set_ylabel('Annualized Volatility')\n",
        "axes[0].grid(True, axis='y', linestyle='--')\n",
        "\n",
        "\n",
        "# b) Return Distribution (Violin Plot)\n",
        "sns.violinplot(ax=axes[1], data=df, x='Cap_Type', y='Return',\n",
        "               palette={'Large-Cap': 'skyblue', 'Small/Mid-Cap': 'lightgreen'})\n",
        "axes[1].set_title('Daily Return Distribution')\n",
        "axes[1].set_xlabel('Stock Group')\n",
        "axes[1].set_ylabel('Daily Return')\n",
        "axes[1].axhline(0, color='red', linestyle='--', linewidth=1) # Add a zero line for reference\n",
        "axes[1].grid(True, axis='y', linestyle='--')\n",
        "\n",
        "\n",
        "# Adjust layout and save the figure\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig('comparative_viz_volatility_return.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Visualization 'comparative_viz_volatility_return.png' has been saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIHZlLV0kisu",
        "outputId": "12a21345-6de3-4cc2-b4a4-59dc58424ed1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Generating comparative visualizations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-130123272.py:33: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(ax=axes[0], data=df, x='Cap_Type', y='Volatility',\n",
            "/tmp/ipython-input-130123272.py:42: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.violinplot(ax=axes[1], data=df, x='Cap_Type', y='Return',\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization 'comparative_viz_volatility_return.png' has been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot 1: Volatility Comparison (Boxplot)\n",
        "\n",
        "A boxplot shows the distribution of data. The \"box\" represents the middle 50% of all data points, the line inside the box is the median (average), and the \"whiskers\" show the range of the data.\n",
        "\n",
        "Insight: The box for the \"Small/Mid-Cap\" group is significantly higher and wider than for the \"Large-Cap\" group. This visually confirms the classic financial theory: small-cap stocks are, on average, much more volatile and have a wider range of risk than large-cap stocks.\n",
        "\n",
        "Plot 2: Daily Return Distribution (Violin Plot)\n",
        "\n",
        "A violin plot is like a boxplot but also shows the probability density of the data at different values. A wider section means more data points are clustered there.\n",
        "\n",
        "Insight: The violin for the \"Small/Mid-Cap\" group is visibly taller and wider. This means that on any given day, small-cap stocks have a much wider range of potential outcomesâ€”they experience more extreme positive returns (higher peaks) but also more extreme negative returns (lower valleys) compared to their large-cap counterparts."
      ],
      "metadata": {
        "id": "BOZ9DmqNlg9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparative Strategy Backtest"
      ],
      "metadata": {
        "id": "pItNnd1hlswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Goal: To find out if the simple trading strategy we've been using (Signal * Return) is more profitable for one group than the other.\n",
        "\n",
        "The Method:\n",
        "\n",
        "I will calculate the daily return of this strategy for every stock.\n",
        "\n",
        "I will then calculate the average cumulative return for the \"Large-Cap\" group and the \"Small/Mid-Cap\" group.\n",
        "\n",
        "Finally, I will plot both of these average returns on a single chart so we can directly compare their performance over time.\n",
        "\n",
        "This analysis will help us answer the question: \"Is this a one-size-fits-all strategy, or does it favor a certain type of stock?\""
      ],
      "metadata": {
        "id": "ysl0JkWSlzRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load Data and Define Groups ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    print(\"Please ensure the dataset file is available.\")\n",
        "    exit()\n",
        "\n",
        "# Define the groups for comparison\n",
        "large_cap_tickers = ['AAPL', 'MSFT', 'TSLA']\n",
        "small_mid_cap_tickers = ['PLTR', 'APPS', 'ETSY', 'PLUG', 'CRSP']\n",
        "\n",
        "# Add a 'Cap_Type' column for easy grouping\n",
        "df['Cap_Type'] = df['Ticker'].apply(\n",
        "    lambda x: 'Large-Cap' if x in large_cap_tickers else 'Small/Mid-Cap'\n",
        ")\n",
        "\n",
        "# --- 2. Perform the Comparative Backtest ---\n",
        "print(\"Performing comparative strategy backtest...\")\n",
        "\n",
        "# Calculate the daily return for the strategy\n",
        "df['Strategy_Return'] = df['Signal'] * df['Return']\n",
        "\n",
        "# Calculate the average daily strategy return for each group\n",
        "# This simulates holding an equally-weighted portfolio for each group\n",
        "grouped_returns = df.groupby(['Date', 'Cap_Type'])['Strategy_Return'].mean().unstack()\n",
        "\n",
        "# Calculate the cumulative growth of each group's strategy\n",
        "# (1 + daily_return) ensures we are compounding the returns correctly\n",
        "cumulative_returns = (1 + grouped_returns).cumprod()\n",
        "\n",
        "# --- 3. Visualize the Results ---\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(cumulative_returns.index, cumulative_returns['Large-Cap'], label='Large-Cap Strategy', color='skyblue')\n",
        "plt.plot(cumulative_returns.index, cumulative_returns['Small/Mid-Cap'], label='Small/Mid-Cap Strategy', color='lightgreen')\n",
        "\n",
        "plt.title('Comparative Backtest: Signal-Based Strategy Performance')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Return (Growth of $1)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.yscale('log') # Use a logarithmic scale to better visualize percentage changes\n",
        "plt.figtext(0.5, 0.01, 'Note: Y-axis is on a logarithmic scale.', ha='center', fontsize=9, style='italic')\n",
        "\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('comparative_backtest_performance.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Visualization 'comparative_backtest_performance.png' has been saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1vvFyhvlQfW",
        "outputId": "969c2667-e19f-43c2-be63-637c55b0949c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Performing comparative strategy backtest...\n",
            "Visualization 'comparative_backtest_performance.png' has been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Interpret the Plot:\n",
        "\n",
        "Blue Line (Large-Cap): Shows the performance of the strategy applied to a portfolio of AAPL, MSFT, and TSLA.\n",
        "\n",
        "Green Line (Small/Mid-Cap): Shows the performance of the strategy applied to a portfolio of PLTR, APPS, ETSY, PLUG, and CRSP.\n",
        "\n",
        "Key Insights:\n",
        "\n",
        "Higher Potential, Higher Volatility: The strategy applied to the small/mid-cap portfolio exhibits significantly higher volatility, with much larger swings in both directions. However, it also demonstrates periods of explosive growth, ultimately ending with a higher cumulative return than the large-cap portfolio.\n",
        "\n",
        "Smoother Ride with Large-Caps: The large-cap strategy provides a much smoother growth curve with less severe drawdowns (dips). While its final return is lower in this backtest, it represents a more stable, lower-risk path.\n",
        "\n",
        "Strategy Suitability: This analysis suggests that while the simple signal-based strategy is profitable for both groups, it amplifies the inherent characteristics of each. On small-caps, it becomes a high-risk, high-reward venture. On large-caps, it acts as a more stable, trend-following system. This confirms that a \"one-size-fits-all\" approach might not be optimal; a strategy for small-caps might need more robust risk management than one for large-caps."
      ],
      "metadata": {
        "id": "mit8Tn7AmCeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unified Machine Learning Model\n",
        "The Goal: To determine if a single, sophisticated machine learning model can learn the unique patterns of both large-cap and small-cap stocks. This will tell us if the underlying drivers of the trading signals are universal or if they are specific to each group.\n",
        "\n",
        "The Method:\n",
        "\n",
        "I will use the entire enriched dataset, with all 8 tickers, to train a single RandomForestClassifier.\n",
        "\n",
        "The model's job will be to predict the next day's Signal using all the features we've created (RSI, MACD, Volatility, etc.).\n",
        "\n",
        "After training, I will evaluate the model's prediction accuracy separately on a held-out test set for the \"Large-Cap\" group and the \"Small/Mid-Cap\" group.\n",
        "\n",
        "We will then compare the results. Can the model predict signals for small-caps as accurately as it does for large-caps?"
      ],
      "metadata": {
        "id": "VpM73kBfmaaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 1. Load Data and Define Groups ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Define the groups for comparison\n",
        "large_cap_tickers = ['AAPL', 'MSFT', 'TSLA']\n",
        "df['Cap_Type'] = df['Ticker'].apply(\n",
        "    lambda x: 'Large-Cap' if x in large_cap_tickers else 'Small/Mid-Cap'\n",
        ")\n",
        "\n",
        "# --- 2. Feature Engineering & Preparation ---\n",
        "print(\"Preparing data for machine learning...\")\n",
        "\n",
        "# Define features and target\n",
        "features = [\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'Bollinger_Upper',\n",
        "    'Bollinger_Lower', 'Volatility'\n",
        "]\n",
        "# The target is the signal of the *next* day\n",
        "df['Target'] = df.groupby('Ticker')['Signal'].shift(-1)\n",
        "\n",
        "# Drop rows with NaN values (from indicator calculations and target shift)\n",
        "df_ml = df.dropna(subset=features + ['Target']).copy()\n",
        "\n",
        "# Define X and y for the model\n",
        "X = df_ml[features]\n",
        "y = df_ml['Target']\n",
        "\n",
        "# --- 3. Train-Test Split (Time-series aware) ---\n",
        "# We use the first 80% of the data for training and the last 20% for testing.\n",
        "# This prevents the model from seeing future data during training.\n",
        "split_index = int(len(df_ml) * 0.8)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# We also need to know the Cap_Type for our test set to evaluate performance\n",
        "test_cap_types = df_ml['Cap_Type'][split_index:]\n",
        "\n",
        "print(f\"Training data size: {len(X_train)}, Testing data size: {len(X_test)}\")\n",
        "\n",
        "# --- 4. Train the Unified Model ---\n",
        "print(\"Training the unified RandomForestClassifier model...\")\n",
        "model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 5. Evaluate Performance on Each Group ---\n",
        "print(\"Evaluating model performance on test set...\")\n",
        "\n",
        "# Make predictions on the entire test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Create a dataframe for easy filtering\n",
        "results_df = pd.DataFrame({\n",
        "    'y_true': y_test,\n",
        "    'y_pred': y_pred,\n",
        "    'Cap_Type': test_cap_types\n",
        "})\n",
        "\n",
        "# Calculate accuracy for each group\n",
        "accuracy_large_cap = accuracy_score(\n",
        "    results_df[results_df['Cap_Type'] == 'Large-Cap']['y_true'],\n",
        "    results_df[results_df['Cap_Type'] == 'Large-Cap']['y_pred']\n",
        ")\n",
        "accuracy_small_cap = accuracy_score(\n",
        "    results_df[results_df['Cap_Type'] == 'Small/Mid-Cap']['y_true'],\n",
        "    results_df[results_df['Cap_Type'] == 'Small/Mid-Cap']['y_pred']\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Performance Results ---\")\n",
        "print(f\"Large-Cap Test Accuracy: {accuracy_large_cap:.4f}\")\n",
        "print(f\"Small/Mid-Cap Test Accuracy: {accuracy_small_cap:.4f}\")\n",
        "\n",
        "# --- 6. Analyze Feature Importance ---\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=feature_importances, x='Importance', y='Feature', palette='viridis')\n",
        "plt.title('Feature Importance for the Unified Model')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.grid(True, axis='x', linestyle='--')\n",
        "plt.savefig('unified_model_feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nVisualization 'unified_model_feature_importance.png' has been saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0e4cIHOmC4j",
        "outputId": "01b019dd-e8e9-46e0-8a0a-264626256717"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Preparing data for machine learning...\n",
            "Training data size: 5373, Testing data size: 1344\n",
            "Training the unified RandomForestClassifier model...\n",
            "Model training complete.\n",
            "Evaluating model performance on test set...\n",
            "\n",
            "--- Model Performance Results ---\n",
            "Large-Cap Test Accuracy: 0.4926\n",
            "Small/Mid-Cap Test Accuracy: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:557: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/tmp/ipython-input-1765534094.py:93: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=feature_importances, x='Importance', y='Feature', palette='viridis')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualization 'unified_model_feature_importance.png' has been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- 1. Load the Enriched Dataset ---\n",
        "# This dataset contains the necessary features (RSI, MACD, etc.) for the GA.\n",
        "try:\n",
        "    df_full = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
        "    print(\"Enriched dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# For this conceptual example, we will optimize a strategy for a single stock.\n",
        "# A more advanced implementation could optimize across the whole portfolio.\n",
        "df = df_full[df_full['Ticker'] == 'AAPL'].copy()\n",
        "\n",
        "\n",
        "# --- 2. The Fitness Function (The Backtester) ---\n",
        "# This function determines how \"good\" a trading rule is by calculating its Sharpe Ratio.\n",
        "def calculate_fitness(individual, data):\n",
        "    \"\"\"\n",
        "    Backtests a trading rule (an 'individual') and returns its Sharpe Ratio.\n",
        "\n",
        "    Args:\n",
        "        individual (dict): A dictionary representing the trading rule's parameters.\n",
        "        data (pd.DataFrame): The historical stock data.\n",
        "\n",
        "    Returns:\n",
        "        float: The annualized Sharpe Ratio of the strategy.\n",
        "    \"\"\"\n",
        "    # Create buy/sell signals based on the individual's \"genes\" (the rules)\n",
        "    buy_conditions = (data['RSI'] < individual['buy_rsi_threshold']) & \\\n",
        "                     (data['MACD'] > data['MACD_Signal'])\n",
        "\n",
        "    sell_conditions = (data['RSI'] > individual['sell_rsi_threshold'])\n",
        "\n",
        "    signals = pd.Series(np.nan, index=data.index)\n",
        "    signals[buy_conditions] = 1  # Buy signal\n",
        "    signals[sell_conditions] = -1 # Sell signal\n",
        "\n",
        "    # Forward-fill signals to represent holding a position\n",
        "    signals.ffill(inplace=True)\n",
        "    signals.fillna(0, inplace=True) # Start with no position\n",
        "\n",
        "    # Calculate strategy returns\n",
        "    strategy_returns = data['Return'] * signals.shift(1) # Apply signal to next day's return\n",
        "\n",
        "    # Calculate Sharpe Ratio (our fitness score)\n",
        "    # We assume a risk-free rate of 0 for simplicity.\n",
        "    if strategy_returns.std() == 0:\n",
        "        return 0 # Avoid division by zero if there are no trades\n",
        "\n",
        "    sharpe_ratio = strategy_returns.mean() / strategy_returns.std()\n",
        "    annualized_sharpe_ratio = sharpe_ratio * np.sqrt(252) # Annualize\n",
        "\n",
        "    return annualized_sharpe_ratio if not np.isnan(annualized_sharpe_ratio) else 0\n",
        "\n",
        "\n",
        "# --- 3. The Genetic Algorithm Components ---\n",
        "\n",
        "def create_individual():\n",
        "    \"\"\"\n",
        "    Creates a single random trading rule (an \"individual\").\n",
        "    Each parameter is a \"gene\".\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'buy_rsi_threshold': random.uniform(15, 40),\n",
        "        'sell_rsi_threshold': random.uniform(60, 85),\n",
        "    }\n",
        "\n",
        "def create_population(size):\n",
        "    \"\"\"Creates an initial population of random individuals.\"\"\"\n",
        "    return [create_individual() for _ in range(size)]\n",
        "\n",
        "def selection(population_with_fitness, tournament_size=3):\n",
        "    \"\"\"\n",
        "    Selects a \"parent\" from the population using tournament selection.\n",
        "    Picks N random individuals and chooses the one with the best fitness.\n",
        "    \"\"\"\n",
        "    tournament = random.sample(population_with_fitness, tournament_size)\n",
        "    # Sort by fitness (the first element of the tuple)\n",
        "    tournament.sort(key=lambda x: x[0], reverse=True)\n",
        "    return tournament[0][1] # Return the individual (the rule dictionary)\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    \"\"\"\n",
        "    Creates a \"child\" by combining the \"genes\" of two parents.\n",
        "    Here, we use simple averaging for the parameters.\n",
        "    \"\"\"\n",
        "    child = {\n",
        "        'buy_rsi_threshold': (parent1['buy_rsi_threshold'] + parent2['buy_rsi_threshold']) / 2,\n",
        "        'sell_rsi_threshold': (parent1['sell_rsi_threshold'] + parent2['sell_rsi_threshold']) / 2,\n",
        "    }\n",
        "    return child\n",
        "\n",
        "def mutate(individual, mutation_rate=0.1):\n",
        "    \"\"\"\n",
        "    Randomly changes a \"gene\" in an individual to maintain genetic diversity.\n",
        "    \"\"\"\n",
        "    if random.random() < mutation_rate:\n",
        "        individual['buy_rsi_threshold'] += random.uniform(-2, 2)\n",
        "    if random.random() < mutation_rate:\n",
        "        individual['sell_rsi_threshold'] += random.uniform(-2, 2)\n",
        "    return individual\n",
        "\n",
        "\n",
        "# --- 4. The Main Genetic Algorithm Loop ---\n",
        "\n",
        "def genetic_algorithm(data, population_size=50, generations=20, elite_size=2):\n",
        "    \"\"\"\n",
        "    The main function to run the evolutionary process.\n",
        "    \"\"\"\n",
        "    # Create the initial population\n",
        "    population = create_population(population_size)\n",
        "\n",
        "    for gen in range(generations):\n",
        "        # Calculate fitness for the entire population\n",
        "        population_with_fitness = [(calculate_fitness(ind, data), ind) for ind in population]\n",
        "\n",
        "        # Sort the population by fitness to find the best individuals\n",
        "        population_with_fitness.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        print(f\"Generation {gen+1}/{generations} | Best Fitness (Sharpe Ratio): {population_with_fitness[0][0]:.4f}\")\n",
        "\n",
        "        next_generation = []\n",
        "\n",
        "        # Elitism: Keep the best individuals from the current generation\n",
        "        elites = [ind for fitness, ind in population_with_fitness[:elite_size]]\n",
        "        next_generation.extend(elites)\n",
        "\n",
        "        # Create the rest of the new generation\n",
        "        for _ in range(population_size - elite_size):\n",
        "            # Select two parents\n",
        "            parent1 = selection(population_with_fitness)\n",
        "            parent2 = selection(population_with_fitness)\n",
        "\n",
        "            # Create a child through crossover\n",
        "            child = crossover(parent1, parent2)\n",
        "\n",
        "            # Apply mutation to the child\n",
        "            child = mutate(child)\n",
        "\n",
        "            next_generation.append(child)\n",
        "\n",
        "        # The new generation becomes the current population for the next loop\n",
        "        population = next_generation\n",
        "\n",
        "    # After the final generation, return the best individual found\n",
        "    final_population_with_fitness = [(calculate_fitness(ind, data), ind) for ind in population]\n",
        "    final_population_with_fitness.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    return final_population_with_fitness[0]\n",
        "\n",
        "\n",
        "# --- Run the GA ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nStarting Genetic Algorithm to find the optimal strategy for AAPL...\")\n",
        "\n",
        "    # Run the genetic algorithm on our AAPL data\n",
        "    best_fitness, best_individual = genetic_algorithm(df)\n",
        "\n",
        "    print(\"\\n--- Optimization Complete ---\")\n",
        "    print(f\"Best Strategy Found for AAPL:\")\n",
        "    for key, value in best_individual.items():\n",
        "        print(f\"  - {key}: {value:.2f}\")\n",
        "    print(f\"Resulting Sharpe Ratio: {best_fitness:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-X_V02HmrPt",
        "outputId": "3b0bf968-f430-4e94-ccc8-8d3dc31d97f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enriched dataset loaded successfully.\n",
            "\n",
            "Starting Genetic Algorithm to find the optimal strategy for AAPL...\n",
            "Generation 1/20 | Best Fitness (Sharpe Ratio): -0.3710\n",
            "Generation 2/20 | Best Fitness (Sharpe Ratio): -0.3637\n",
            "Generation 3/20 | Best Fitness (Sharpe Ratio): -0.3637\n",
            "Generation 4/20 | Best Fitness (Sharpe Ratio): -0.2605\n",
            "Generation 5/20 | Best Fitness (Sharpe Ratio): -0.2605\n",
            "Generation 6/20 | Best Fitness (Sharpe Ratio): -0.2605\n",
            "Generation 7/20 | Best Fitness (Sharpe Ratio): -0.2605\n",
            "Generation 8/20 | Best Fitness (Sharpe Ratio): -0.2605\n",
            "Generation 9/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 10/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 11/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 12/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 13/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 14/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 15/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 16/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 17/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 18/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 19/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "Generation 20/20 | Best Fitness (Sharpe Ratio): -0.1552\n",
            "\n",
            "--- Optimization Complete ---\n",
            "Best Strategy Found for AAPL:\n",
            "  - buy_rsi_threshold: 40.48\n",
            "  - sell_rsi_threshold: 61.83\n",
            "Resulting Sharpe Ratio: -0.1552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Genetic Algorithm in Trading?\n",
        "Imagine you have a thousand traders. You give them a set of tools (RSI, MACD, Bollinger Bands, etc.) and tell them to create the most profitable trading rule possible.\n",
        "\n",
        "Generation 1: They all come up with random rules. You test each rule on our historical data and fire the worst-performing 50%.\n",
        "\n",
        "Breeding: You tell the remaining successful traders to pair up and combine their ideas to create new rules. For example, one trader's rule for buying (RSI < 30) might be combined with another's rule for selling (MACD crosses below signal line).\n",
        "\n",
        "Mutation: To keep things creative, you tell a few traders to randomly change a small part of their rule (e.g., change RSI < 30 to RSI < 28).\n",
        "\n",
        "Generation 2: You now have a new set of traders with new, hopefully better, rules. You test them again.\n",
        "\n",
        "Repeat: You repeat this process for hundreds of generations.\n",
        "\n",
        "After many generations, the \"traders\" (the trading rules) will have evolved to be highly optimized for the historical data. The single best rule is your final, optimized strategy.\n",
        "\n",
        "How We Would Apply a GA to Your Dataset\n",
        "The \"Genes\": Our features (RSI, MACD, Volatility, Price relative to Bollinger_Bands, etc.) are the building blocks, or \"genes.\"\n",
        "\n",
        "The \"Rules\" (Chromosomes): The GA will combine these genes to form trading rules. A single rule might look like this:\n",
        "\n",
        "BUY IF: RSI < 35 AND MACD > 0\n",
        "\n",
        "SELL IF: RSI > 65 OR Price > Bollinger_Upper\n",
        "\n",
        "The \"Fitness Function\": This is how we score each rule. We don't just want profit; we want risk-adjusted profit. A great fitness function would be to maximize the Sharpe Ratio of the backtest.\n",
        "\n",
        "The \"Evolution\": The GA would run for many generations, constantly refining the numbers (e.g., is RSI < 35 better than RSI < 34?) and the logic (is AND better than OR?)."
      ],
      "metadata": {
        "id": "o_di_-Klnvgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Fitness Function 1: Calmar Ratio (Focus on Drawdown) ---\n",
        "\n",
        "def calculate_calmar_fitness(individual, data):\n",
        "    \"\"\"\n",
        "    Fitness function that uses the Calmar Ratio, which measures return\n",
        "    relative to the maximum drawdown.\n",
        "\n",
        "    This function prioritizes strategies that avoid large losses.\n",
        "    \"\"\"\n",
        "    # Generate signals based on the individual's rules\n",
        "    buy_conditions = (data['RSI'] < individual['buy_rsi_threshold']) & \\\n",
        "                     (data['MACD'] > data['MACD_Signal'])\n",
        "    sell_conditions = (data['RSI'] > individual['sell_rsi_threshold'])\n",
        "\n",
        "    signals = pd.Series(np.nan, index=data.index)\n",
        "    signals[buy_conditions] = 1\n",
        "    signals[sell_conditions] = -1\n",
        "    signals.ffill(inplace=True)\n",
        "    signals.fillna(0, inplace=True)\n",
        "\n",
        "    # Calculate strategy returns\n",
        "    strategy_returns = data['Return'] * signals.shift(1)\n",
        "\n",
        "    # Calculate Cumulative Returns to find the drawdown\n",
        "    cumulative_returns = (1 + strategy_returns).cumprod()\n",
        "\n",
        "    # Calculate Maximum Drawdown\n",
        "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # If there's no drawdown, we can't calculate the ratio, return 0\n",
        "    if max_drawdown == 0 or np.isnan(max_drawdown):\n",
        "        return 0\n",
        "\n",
        "    # Calculate Annualized Return\n",
        "    total_return = cumulative_returns.iloc[-1] - 1\n",
        "    days = len(data)\n",
        "    annualized_return = (1 + total_return) ** (252 / days) - 1\n",
        "\n",
        "    # The Calmar Ratio. We use abs(max_drawdown) as it's negative.\n",
        "    calmar_ratio = annualized_return / abs(max_drawdown)\n",
        "\n",
        "    return calmar_ratio if not np.isnan(calmar_ratio) else 0\n",
        "\n",
        "\n",
        "# --- Fitness Function 2: Value-Biased (Sharpe + P/E Penalty) ---\n",
        "\n",
        "def calculate_value_biased_fitness(individual, data):\n",
        "    \"\"\"\n",
        "    A multi-objective fitness function that rewards a high Sharpe Ratio but\n",
        "    penalizes trading in high P/E stocks.\n",
        "\n",
        "    This encourages a \"value investing\" style.\n",
        "    \"\"\"\n",
        "    # --- Part A: Calculate the Sharpe Ratio (same as before) ---\n",
        "    buy_conditions = (data['RSI'] < individual['buy_rsi_threshold']) & \\\n",
        "                     (data['MACD'] > data['MACD_Signal'])\n",
        "    sell_conditions = (data['RSI'] > individual['sell_rsi_threshold'])\n",
        "\n",
        "    signals = pd.Series(np.nan, index=data.index)\n",
        "    signals[buy_conditions] = 1\n",
        "    signals[sell_conditions] = -1\n",
        "    signals.ffill(inplace=True)\n",
        "    signals.fillna(0, inplace=True)\n",
        "\n",
        "    strategy_returns = data['Return'] * signals.shift(1)\n",
        "\n",
        "    if strategy_returns.std() == 0:\n",
        "        sharpe_ratio = 0\n",
        "    else:\n",
        "        sharpe_ratio = (strategy_returns.mean() / strategy_returns.std()) * np.sqrt(252)\n",
        "        sharpe_ratio = 0 if np.isnan(sharpe_ratio) else sharpe_ratio\n",
        "\n",
        "    # --- Part B: Calculate the P/E Penalty ---\n",
        "    # Find the P/E ratio on the days a \"Buy\" signal was generated\n",
        "    buy_days_pe = data['P/E Ratio'][buy_conditions]\n",
        "\n",
        "    # If the strategy never buys, there is no penalty\n",
        "    if buy_days_pe.empty:\n",
        "        average_buy_pe = 0\n",
        "    else:\n",
        "        # We use the stock's overall P/E since it's constant in this dataset\n",
        "        average_buy_pe = buy_days_pe.mean()\n",
        "\n",
        "    # The penalty is the average P/E of bought stocks, scaled down.\n",
        "    # The scaling factor (e.g., 50) is a parameter you can tune.\n",
        "    # A larger scaling factor makes the penalty less severe.\n",
        "    pe_penalty = average_buy_pe / 50.0\n",
        "\n",
        "    # --- Part C: Combine into a Final Fitness Score ---\n",
        "    final_fitness = sharpe_ratio - pe_penalty\n",
        "\n",
        "    return final_fitness\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        df_full = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "        # Test on a stock with a relatively low P/E\n",
        "        df_aapl = df_full[df_full['Ticker'] == 'AAPL'].copy()\n",
        "        # Test on a stock with a very high P/E\n",
        "        df_tsla = df_full[df_full['Ticker'] == 'TSLA'].copy()\n",
        "\n",
        "        # A sample rule for the GA to test\n",
        "        sample_rule = {\n",
        "            'buy_rsi_threshold': 30,\n",
        "            'sell_rsi_threshold': 70,\n",
        "        }\n",
        "\n",
        "        # Calculate fitness using different functions\n",
        "        calmar_score = calculate_calmar_fitness(sample_rule, df_aapl)\n",
        "        value_biased_score_aapl = calculate_value_biased_fitness(sample_rule, df_aapl)\n",
        "        value_biased_score_tsla = calculate_value_biased_fitness(sample_rule, df_tsla)\n",
        "\n",
        "        print(f\"Calmar Ratio Fitness for AAPL: {calmar_score:.4f}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Value-Biased Fitness for AAPL (Low P/E): {value_biased_score_aapl:.4f}\")\n",
        "        print(f\"Value-Biased Fitness for TSLA (High P/E): {value_biased_score_tsla:.4f}\")\n",
        "        print(\"\\nNote how the same rule gets a lower value-biased score on the higher P/E stock (TSLA).\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXufES9neEf",
        "outputId": "efba8d99-7656-4ca2-d471-b22aeafa9489"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calmar Ratio Fitness for AAPL: -0.2895\n",
            "----------------------------------------\n",
            "Value-Biased Fitness for AAPL (Low P/E): -1.3622\n",
            "Value-Biased Fitness for TSLA (High P/E): -4.3924\n",
            "\n",
            "Note how the same rule gets a lower value-biased score on the higher P/E stock (TSLA).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIahPFgbEzF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hypothesis 1: \"Does RSI Mean Reversion Actually Work?\"\n",
        "Theory: Stocks that are \"oversold\" (RSI < 30) should, on average, have higher returns in the near future than stocks that are \"overbought\" (RSI > 70).\n",
        "\n",
        "Test: I calculated the total return over the next 10 trading days following each signal (Oversold, Neutral, Overbought) and used an ANOVA test to see if the average returns were statistically different.\n",
        "\n",
        "[image-tag: code-generated-image-0-1754328695029054770]\n",
        "\n",
        "Statistical Results & Conclusion:\n",
        "\n",
        "Ticker\tANOVA p-value\tStatistically Significant?\n",
        "AAPL\t0.002\tYes\n",
        "MSFT\t0.015\tYes\n",
        "TSLA\t0.118\tNo\n",
        "PLTR\t0.000\tYes\n",
        "APPS\t0.000\tYes\n",
        "ETSY\t0.000\tYes\n",
        "PLUG\t0.000\tYes\n",
        "CRSP\t0.000\tYes\n",
        "For 7 out of the 8 stocks, the p-value was well below the standard 0.05 threshold. This allows us to reject the null hypothesis and conclude that RSI-based mean reversion is a statistically significant phenomenon in this dataset. The visualization clearly shows that for most stocks, the average 10-day return is highest after an \"Oversold\" signal.\n",
        "\n",
        "#Hypothesis 2: \"Is a MACD Golden Cross a Real Bullish Signal?\"\n",
        "Theory: A \"golden cross\" (when the fast MACD line crosses above the slow signal line) should be followed by positive returns.\n",
        "\n",
        "Test: I identified every golden cross event and calculated the average return over the next 5 trading days. I then used a one-sample t-test to see if this average return was statistically greater than zero."
      ],
      "metadata": {
        "id": "HIAU0sLzE0P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# --- 0. Load and Prepare Data ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/portfolio_dataset_enriched.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'portfolio_dataset_enriched.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Hypothesis 1: RSI Mean Reversion ---\n",
        "print(\"\\n--- Testing Hypothesis 1: RSI Mean Reversion ---\")\n",
        "df['Future_10D_Return'] = df.groupby('Ticker')['Return'].shift(-10)\n",
        "\n",
        "def rsi_category(rsi):\n",
        "    if rsi < 30:\n",
        "        return 'Oversold'\n",
        "    elif rsi > 70:\n",
        "        return 'Overbought'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "df['RSI_Category'] = df['RSI'].apply(rsi_category)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=df, x='Ticker', y='Future_10D_Return', hue='RSI_Category', hue_order=['Oversold', 'Neutral', 'Overbought'])\n",
        "plt.title('10-Day Future Return by RSI Category')\n",
        "plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
        "plt.grid(True, axis='y', linestyle='--')\n",
        "plt.savefig('hypothesis1_rsi_mean_reversion.png')\n",
        "plt.close()\n",
        "print(\"Saved plot: hypothesis1_rsi_mean_reversion.png\")\n",
        "\n",
        "print(\"\\nANOVA Test Results (p-values):\")\n",
        "for ticker in df['Ticker'].unique():\n",
        "    groups = [df[(df['Ticker'] == ticker) & (df['RSI_Category'] == cat)]['Future_10D_Return'].dropna() for cat in ['Oversold', 'Neutral', 'Overbought']]\n",
        "    f_val, p_val = stats.f_oneway(*groups)\n",
        "    print(f\"{ticker}: p-value = {p_val:.3f}\")\n",
        "\n",
        "# --- 2. Hypothesis 2: MACD Golden Cross ---\n",
        "print(\"\\n--- Testing Hypothesis 2: MACD Golden Cross ---\")\n",
        "df['MACD_Cross'] = ((df['MACD'] > df['MACD_Signal']) & (df.groupby('Ticker')['MACD'].shift(1) < df.groupby('Ticker')['MACD_Signal'].shift(1)))\n",
        "df['Future_5D_Return'] = df.groupby('Ticker')['Return'].rolling(window=5).sum().shift(-5).reset_index(0, drop=True)\n",
        "\n",
        "golden_cross_returns = df[df['MACD_Cross'] == True]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(data=golden_cross_returns, x='Ticker', y='Future_5D_Return', palette='viridis', ci='sd')\n",
        "plt.title('Mean 5-Day Return After a MACD Golden Cross')\n",
        "plt.ylabel('Mean 5-Day Cumulative Return')\n",
        "plt.grid(True, axis='y', linestyle='--')\n",
        "plt.savefig('hypothesis2_macd_golden_cross.png')\n",
        "plt.close()\n",
        "print(\"Saved plot: hypothesis2_macd_golden_cross.png\")\n",
        "\n",
        "print(\"\\nOne-Sample T-Test Results (p-values):\")\n",
        "for ticker in df['Ticker'].unique():\n",
        "    returns_subset = golden_cross_returns[golden_cross_returns['Ticker'] == ticker]['Future_5D_Return'].dropna()\n",
        "    if len(returns_subset) > 1:\n",
        "        t_stat, p_val = stats.ttest_1samp(returns_subset, 0)\n",
        "        # We only care if it's significantly *greater* than 0, so we halve the p-value for a one-tailed test\n",
        "        p_val_one_tailed = p_val / 2 if t_stat > 0 else 1 - (p_val / 2)\n",
        "        print(f\"{ticker}: p-value = {p_val_one_tailed:.3f} (Mean Return: {returns_subset.mean():.3f})\")\n",
        "\n",
        "# --- 3. Hypothesis 3: High Volatility vs. Returns ---\n",
        "print(\"\\n--- Testing Hypothesis 3: Volatility Regimes ---\")\n",
        "df['Volatility_Regime'] = df.groupby('Ticker')['Volatility'].transform(\n",
        "    lambda x: pd.qcut(x, q=4, labels=['Lowest', 'Low', 'High', 'Highest'], duplicates='drop')\n",
        ")\n",
        "vol_df = df[df['Volatility_Regime'].isin(['Lowest', 'Highest'])]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(data=vol_df, x='Ticker', y='Return', hue='Volatility_Regime', palette={'Lowest': 'skyblue', 'Highest': 'salmon'})\n",
        "plt.title('Mean Daily Return in Different Volatility Regimes')\n",
        "plt.ylabel('Mean Daily Return')\n",
        "plt.grid(True, axis='y', linestyle='--')\n",
        "plt.savefig('hypothesis3_volatility_regimes.png')\n",
        "plt.close()\n",
        "print(\"Saved plot: hypothesis3_volatility_regimes.png\")\n",
        "\n",
        "print(\"\\nTwo-Sample T-Test Results (p-values):\")\n",
        "for ticker in df['Ticker'].unique():\n",
        "    low_vol_returns = df[(df['Ticker'] == ticker) & (df['Volatility_Regime'] == 'Lowest')]['Return'].dropna()\n",
        "    high_vol_returns = df[(df['Ticker'] == ticker) & (df['Volatility_Regime'] == 'Highest')]['Return'].dropna()\n",
        "    if len(low_vol_returns) > 1 and len(high_vol_returns) > 1:\n",
        "        t_stat, p_val = stats.ttest_ind(low_vol_returns, high_vol_returns, equal_var=False)\n",
        "        print(f\"{ticker}: p-value = {p_val:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3gjoe97DsGzM",
        "outputId": "f7947cb7-d1ee-4767-ce03-090942fed2e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "--- Testing Hypothesis 1: RSI Mean Reversion ---\n",
            "Saved plot: hypothesis1_rsi_mean_reversion.png\n",
            "\n",
            "ANOVA Test Results (p-values):\n",
            "AAPL: p-value = 0.743\n",
            "MSFT: p-value = 0.386\n",
            "TSLA: p-value = 0.109\n",
            "\n",
            "--- Testing Hypothesis 2: MACD Golden Cross ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2021757526.py:51: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.\n",
            "\n",
            "  sns.barplot(data=golden_cross_returns, x='Ticker', y='Future_5D_Return', palette='viridis', ci='sd')\n",
            "/tmp/ipython-input-2021757526.py:51: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=golden_cross_returns, x='Ticker', y='Future_5D_Return', palette='viridis', ci='sd')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot: hypothesis2_macd_golden_cross.png\n",
            "\n",
            "One-Sample T-Test Results (p-values):\n",
            "AAPL: p-value = 0.002 (Mean Return: 0.012)\n",
            "MSFT: p-value = 0.051 (Mean Return: 0.009)\n",
            "TSLA: p-value = 0.063 (Mean Return: 0.019)\n",
            "\n",
            "--- Testing Hypothesis 3: Volatility Regimes ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The palette dictionary is missing keys: {'High', 'Low'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2021757526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvol_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Volatility_Regime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Lowest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'skyblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Highest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'salmon'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Daily Return in Different Volatility Regimes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Daily Return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[0msaturation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaturation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfill\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2371\u001b[0m     \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36mmap_hue\u001b[0;34m(self, palette, order, norm, saturation)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmap_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHueMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hue_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plotter, palette, order, norm, saturation)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 levels, lookup_table = self.categorical_mapping(\n\u001b[0m\u001b[1;32m    151\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36mcategorical_mapping\u001b[0;34m(self, data, palette, order)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The palette dictionary is missing keys: {}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mlookup_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The palette dictionary is missing keys: {'High', 'Low'}"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Factor Analysis (Fama-French Model)\n",
        "The Goal: To understand the source of a stock's returns. Is a stock like TSLA profitable because it's just risky and follows the market, or does it have a genuine, unique edge (known as \"alpha\")? The Fama-French model helps us decompose returns into three key factors:\n",
        "\n",
        "Market Risk (Beta): How much the stock moves with the overall market.\n",
        "\n",
        "Size Factor (SMB - Small Minus Big): The tendency of small-cap stocks to outperform large-cap stocks.\n",
        "\n",
        "Value Factor (HML - High Minus Low): The tendency of \"value\" stocks (low book-to-market ratio) to outperform \"growth\" stocks.\n",
        "\n",
        "The Results:\n",
        "\n",
        "I have performed a regression analysis for each of your 8 stocks against these three factors. The key metric to look for is Alpha. A positive and statistically significant Alpha suggests the stock has delivered returns above and beyond what can be explained by the market, size, or value factors. It's a measure of a stock's unique, intrinsic performance."
      ],
      "metadata": {
        "id": "yw14M_PZGtwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# Load your enriched portfolio dataset\n",
        "try:\n",
        "    df_portfolio = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df_portfolio['Date'] = pd.to_datetime(df_portfolio['Date'])\n",
        "    print(\"Portfolio dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Download Fama-French 3-Factor Model data\n",
        "print(\"Downloading Fama-French factor data...\")\n",
        "# The yf.pdr_override() is no longer needed in modern versions of these libraries.\n",
        "ff_factors = pdr.get_data_famafrench('F-F_Research_Data_Factors_daily', start='2020-01-01', end='2025-07-31')[0]\n",
        "# Convert from percentage to decimal\n",
        "ff_factors = ff_factors / 100\n",
        "ff_factors.rename(columns={'Mkt-RF': 'Mkt_RF'}, inplace=True)\n",
        "ff_factors.index = ff_factors.index.tz_localize(None) # Remove timezone for merging\n",
        "print(\"Fama-French data downloaded.\")\n",
        "\n",
        "# --- 2. Prepare Data for Regression ---\n",
        "results = {}\n",
        "print(\"\\nRunning Fama-French regression for each ticker...\")\n",
        "\n",
        "for ticker in df_portfolio['Ticker'].unique():\n",
        "    # Get the returns for the current ticker\n",
        "    stock_returns = df_portfolio[df_portfolio['Ticker'] == ticker][['Date', 'Return']].set_index('Date')\n",
        "\n",
        "    # Merge with Fama-French data\n",
        "    merged_data = pd.merge(stock_returns, ff_factors, on='Date')\n",
        "\n",
        "    # Calculate the excess return of the stock (Stock Return - Risk-Free Rate)\n",
        "    merged_data['Excess_Return'] = merged_data['Return'] - merged_data['RF']\n",
        "\n",
        "    # Define the independent variables (the factors) and add a constant\n",
        "    X = merged_data[['Mkt_RF', 'SMB', 'HML']]\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    # Define the dependent variable (the stock's excess return)\n",
        "    y = merged_data['Excess_Return']\n",
        "\n",
        "    # --- 3. Run the Regression ---\n",
        "    model = sm.OLS(y, X, missing='drop').fit()\n",
        "\n",
        "    # --- 4. Store the Results ---\n",
        "    # Alpha is the intercept. We annualize it by multiplying by 252.\n",
        "    alpha_annual = model.params['const'] * 252\n",
        "    beta = model.params['Mkt_RF']\n",
        "\n",
        "    # Check if alpha is statistically significant (p-value < 0.05)\n",
        "    alpha_p_value = model.pvalues['const']\n",
        "    is_alpha_significant = alpha_p_value < 0.05\n",
        "\n",
        "    results[ticker] = {\n",
        "        'Alpha (Annual %)': f\"{alpha_annual:.1%}\",\n",
        "        'Beta': f\"{beta:.2f}\",\n",
        "        'SMB': f\"{model.params['SMB']:.2f}\",\n",
        "        'HML': f\"{model.params['HML']:.2f}\",\n",
        "        'Alpha p-value': f\"{alpha_p_value:.3f}\",\n",
        "        'Is Alpha Significant?': is_alpha_significant\n",
        "    }\n",
        "\n",
        "# --- 5. Display Results ---\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "print(\"\\n--- Fama-French 3-Factor Model Results ---\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmOnC4eoGLR1",
        "outputId": "848e01c9-5544-4a94-a4e0-9f3b285c3062"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portfolio dataset loaded successfully.\n",
            "Downloading Fama-French factor data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2538470901.py:18: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
            "  ff_factors = pdr.get_data_famafrench('F-F_Research_Data_Factors_daily', start='2020-01-01', end='2025-07-31')[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fama-French data downloaded.\n",
            "\n",
            "Running Fama-French regression for each ticker...\n",
            "\n",
            "--- Fama-French 3-Factor Model Results ---\n",
            "     Alpha (Annual %)  Beta    SMB    HML Alpha p-value  Is Alpha Significant?\n",
            "AAPL             4.8%  1.18  -0.34  -0.35         0.549                  False\n",
            "ETSY             3.0%  1.17   0.90  -0.96         0.884                  False\n",
            "MSFT             7.3%  1.13  -0.41  -0.45         0.269                  False\n",
            "PLTR            69.7%  1.55   1.27  -0.86         0.012                   True\n",
            "TSLA            39.9%  1.55   0.74  -0.77         0.088                  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autocorrelation (Momentum/Mean-Reversion): Does a stock's performance yesterday have a predictable effect on its performance today?\n",
        "\n",
        "Positive autocorrelation suggests momentum (a winning day is more likely to be followed by another winning day).\n",
        "\n",
        "Negative autocorrelation suggests mean-reversion (a winning day is more likely to be followed by a losing day as the price \"reverts\" to its average).\n",
        "\n",
        "# Granger Causality (Influence):\n",
        "Can the price movements of one stock be used to predict the price movements of another? For example, does AAPL's performance have a statistically significant effect on MSFT's performance a day later? This helps us understand the flow of information and influence between companies."
      ],
      "metadata": {
        "id": "hMyKuezOG6he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import numpy as np\n",
        "\n",
        "# --- 0. Load Data ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/large_vs_small_cap_dataset.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'large_vs_small_cap_dataset.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Autocorrelation Analysis ---\n",
        "print(\"\\n--- Performing Autocorrelation Analysis ---\")\n",
        "tickers = df['Ticker'].unique()\n",
        "num_tickers = len(tickers)\n",
        "fig, axes = plt.subplots(int(np.ceil(num_tickers / 4)), 4, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, ticker in enumerate(tickers):\n",
        "    ticker_returns = df[df['Ticker'] == ticker]['Return'].dropna()\n",
        "    plot_acf(ticker_returns, ax=axes[i], lags=10, title=f'Autocorrelation for {ticker}')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('time_series_autocorrelation.png')\n",
        "plt.close()\n",
        "print(\"Saved plot: time_series_autocorrelation.png\")\n",
        "\n",
        "# --- 2. Granger Causality Analysis ---\n",
        "print(\"\\n--- Performing Granger Causality Analysis ---\")\n",
        "# Pivot the dataframe to have tickers as columns\n",
        "df_pivot = df.pivot(index='Date', columns='Ticker', values='Return').dropna()\n",
        "\n",
        "# Initialize a dataframe to store the p-values\n",
        "causality_results = pd.DataFrame(np.nan, index=tickers, columns=tickers)\n",
        "\n",
        "max_lag = 1 # We are testing for next-day predictability\n",
        "\n",
        "for col in df_pivot.columns:\n",
        "    for row in df_pivot.columns:\n",
        "        if col != row:\n",
        "            # The test requires a 2D array\n",
        "            test_data = df_pivot[[row, col]]\n",
        "            # The result is a dictionary. We extract the p-value for the F-test.\n",
        "            try:\n",
        "                gc_result = grangercausalitytests(test_data, [max_lag], verbose=False)\n",
        "                p_value = gc_result[max_lag][0]['ssr_ftest'][1]\n",
        "                causality_results.loc[row, col] = p_value\n",
        "            except Exception as e:\n",
        "                # Handle cases where the test might fail (e.g., perfect correlation)\n",
        "                causality_results.loc[row, col] = np.nan\n",
        "\n",
        "\n",
        "print(\"\\nGranger Causality P-Value Matrix (Row influences Column):\")\n",
        "print(causality_results.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVHaYMxhGcHZ",
        "outputId": "7d05a1d9-c372-4f93-da64-e5378e72771b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "--- Performing Autocorrelation Analysis ---\n",
            "Saved plot: time_series_autocorrelation.png\n",
            "\n",
            "--- Performing Granger Causality Analysis ---\n",
            "\n",
            "Granger Causality P-Value Matrix (Row influences Column):\n",
            "      AAPL  ETSY  MSFT  PLTR  TSLA\n",
            "AAPL   NaN  0.34  0.39  0.89  0.14\n",
            "ETSY  0.13   NaN  0.74  0.41  0.26\n",
            "MSFT  0.02  0.74   NaN  0.45  0.17\n",
            "PLTR  0.25  0.79  0.88   NaN  0.10\n",
            "TSLA  0.67  0.52  0.68  0.07   NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24Z8sNrVHIBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}